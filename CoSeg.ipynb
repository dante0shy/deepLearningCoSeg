{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePBrfsWGB2yf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import PIL.Image as Image\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from torchvision.transforms import Compose, CenterCrop, Normalize, Scale, Resize\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from torch.utils.data import DataLoader\n",
    "# from model import *\n",
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg16\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSAEjIIXCqED"
   },
   "outputs": [],
   "source": [
    "#mian structure of our network\n",
    "# compute region correlation \n",
    "class non_local_block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(non_local_block, self).__init__()\n",
    "\n",
    "        self.mlp1 = nn.Linear(512, 4096)\n",
    "        self.mlp2 = nn.Linear(4096, 512)\n",
    "        self.theta = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0)\n",
    "        self.phi = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv1x1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1, padding=0)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=4, stride=4)\n",
    "\n",
    "        self.conv_y = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv_lastlayer=nn.Conv2d(in_channels=1024,out_channels=512,kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "        self.pool_y=nn.AdaptiveAvgPool2d((2,2))\n",
    "\n",
    "    def forward(self, featureX,featureY):\n",
    "        #加pool\n",
    "        featureY=self.pool_y(featureY)\n",
    "\n",
    "        batch_size = featureX.size(0)  # N\n",
    "        channel_size = featureX.size(1)\n",
    "\n",
    "        theta_x = self.theta(featureX).view(batch_size, channel_size // 2, -1)  # (batch,C//2,H*W)\n",
    "        theta_x = theta_x.permute(0, 2, 1)  # (batch,H2*W2,C//2)\n",
    "\n",
    "        phi_y1 = self.phi(featureY).view(batch_size, channel_size // 2, -1)  # (batch,C//2,7*7)\n",
    "        f1 = torch.matmul(theta_x, phi_y1)  # (batch,H*W,7*7)\n",
    "        f_div_C1 = F.softmax(f1, dim=-1)  # normalize the last dim by softmax\n",
    "        featureY = featureY.view(batch_size, channel_size, -1)  # N,512,7*7\n",
    "        featureY = featureY.permute(0, 2, 1)  # N,7*7,512\n",
    "        y1 = torch.matmul(f_div_C1, featureY)  # batch,H*W,C\n",
    "        y1 = y1.permute(0, 2, 1).contiguous()\n",
    "        # y1 = y1.view(batch_size, channel_size, 16, 16).permute(0, 2, 3, 1)  # batch,16,16,512\n",
    "        # # experiment12 对比下linear和1x1卷积\n",
    "        # W_y1 = self.mlp2(F.tanh(self.mlp1(y1))).permute(0, 3, 1, 2)  # batch,512,16,16\n",
    "        y1=y1.view(batch_size,channel_size,16,16)   # batch,512,16,16\n",
    "\n",
    "        return y1\n",
    "\n",
    "        # out=torch.cat((W_y1,featureX),dim=1)\n",
    "        # out=F.relu(self.conv_lastlayer(out))\n",
    "        # # out=W_y1*featureX\n",
    "        #\n",
    "        # return out\n",
    "\n",
    "\n",
    "# Define some constants\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1:\n",
    "        #print(classname)\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "# LSTM to optimize\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Generate a convolutional LSTM cell\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size,kernel_size=3,dilation=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size#input channel\n",
    "        self.hidden_size = hidden_size#hidden channel\n",
    "        self.Gates = nn.Conv2d(in_channels=input_size + hidden_size,\n",
    "                               out_channels= 4 * hidden_size,\n",
    "                               kernel_size=kernel_size,\n",
    "                               dilation=dilation,\n",
    "                               padding=(kernel_size-3)//2+dilation)\n",
    "        self.Gates.apply(weights_init)\n",
    "\n",
    "\n",
    "    def forward(self, input_, prev_state):\n",
    "\n",
    "        # get batch and spatial sizes\n",
    "        batch_size = input_.data.size()[0]\n",
    "        spatial_size = input_.data.size()[2:]\n",
    "\n",
    "        # generate empty prev_state, if None is provided\n",
    "        if prev_state is None:\n",
    "            state_size = [batch_size, self.hidden_size] + list(spatial_size)\n",
    "            prev_state = (\n",
    "                torch.zeros(state_size),\n",
    "                torch.zeros(state_size)\n",
    "            )\n",
    "\n",
    "        prev_hidden, prev_cell = prev_state#previous state\n",
    "\n",
    "        # data size is [batch, channel, height, width]\n",
    "        stacked_inputs = torch.cat((input_, prev_hidden), 1)\n",
    "        gates = self.Gates(stacked_inputs)\n",
    "        #print (gates.shape)\n",
    "\n",
    "        # chunk across channel dimension\n",
    "        in_gate, remember_gate, out_gate, cell_gate = gates.chunk(4, 1)\n",
    "\n",
    "        # apply sigmoid non linearity\n",
    "        in_gate = torch.sigmoid(in_gate)\n",
    "        remember_gate = torch.sigmoid(remember_gate)\n",
    "        out_gate = torch.sigmoid(out_gate)\n",
    "\n",
    "        # apply tanh non linearity\n",
    "        cell_gate = torch.tanh(cell_gate)\n",
    "\n",
    "        # compute current cell and hidden state\n",
    "        cell = (remember_gate * prev_cell) + (in_gate * cell_gate)\n",
    "        hidden = out_gate * torch.tanh(cell)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "# our model\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model,self).__init__()\n",
    "        self.pretrained_model=vgg16(pretrained=True)\n",
    "        self.features,self.classifiers=list(self.pretrained_model.features.children()),list(self.pretrained_model.classifier.children())\n",
    "\n",
    "        self.features_map=nn.Sequential(*self.features)\n",
    "        self.global_avg_pool=nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.mlp1=nn.Linear(512,4096)\n",
    "        self.mlp2=nn.Linear(4096,512)\n",
    "        self.upsample=nn.Upsample(16)\n",
    "        self.dec = Decoder(2, 512, 2, activ='relu', pad_type='reflect')\n",
    "\n",
    "        self.lstm_cell = ConvLSTMCell(512, 512, kernel_size=3, dilation=1)\n",
    "        self.conv=nn.Conv2d(in_channels=512,out_channels=512,kernel_size=1,stride=1)\n",
    "\n",
    "        self.non_local_block1=non_local_block()\n",
    "        self.conv_last=nn.Conv2d(in_channels=1024,out_channels=512,kernel_size=1)\n",
    "        self.iteration=4\n",
    "\n",
    "    def forward(self, x,y):\n",
    "        vgg_x,vgg_y,vgg_x_weight,vgg_y_weight = self.encode(x,y)\n",
    "        images_recon_x,images_recon_y = self.decode(vgg_x_weight,vgg_y_weight)\n",
    "        return images_recon_x,images_recon_y\n",
    "\n",
    "\n",
    "    def encode(self, x,y):\n",
    "        vgg_x = self.features_map(x)\n",
    "        vgg_y = self.features_map(y)\n",
    "\n",
    "        x_input=self.upsample(self.global_avg_pool(vgg_y))\n",
    "        y_input=self.upsample(self.global_avg_pool(vgg_x))\n",
    "\n",
    "        hidden_state_x = vgg_x\n",
    "        cell_x = vgg_x\n",
    "\n",
    "        hidden_state_y = vgg_y\n",
    "        cell_y = vgg_y\n",
    "\n",
    "        for i in range(self.iteration):\n",
    "            hidden_state_x, cell_x = self.lstm_cell(x_input, (hidden_state_x, cell_x))\n",
    "            hidden_state_y, cell_y = self.lstm_cell(y_input, (hidden_state_y, cell_y))\n",
    "\n",
    "            non_local_x=self.non_local_block1(cell_x,cell_y)\n",
    "            non_local_y=self.non_local_block1(cell_y,cell_x)\n",
    "\n",
    "            x_input=self.upsample(self.global_avg_pool(cell_y))\n",
    "            x_input=(x_input+non_local_x)/2\n",
    "            y_input=self.upsample(self.global_avg_pool(cell_x))\n",
    "            y_input=(y_input+non_local_y)/2\n",
    "\n",
    "        # vgg_x_weight = self.global_avg_pool(vgg_x)\n",
    "        # vgg_x_weight = self.upsample(F.softmax(self.mlp2(F.tanh(self.mlp1(vgg_x_weight.view(-1,512)))),dim=-1).view(-1,512,1,1))\n",
    "        #\n",
    "        # vgg_y_weight = self.global_avg_pool(vgg_y)\n",
    "        # vgg_y_weight = self.upsample(F.softmax(self.mlp2(F.tanh(self.mlp1(vgg_y_weight.view(-1,512)))),dim=-1).view(-1,512,1,1))\n",
    "        return vgg_x,vgg_y,hidden_state_x,hidden_state_y\n",
    "\n",
    "    def decode(self,  vgg_x_weight, vgg_y_weight):\n",
    "        vgg_x_weight=F.relu(self.conv(vgg_x_weight))\n",
    "        vgg_y_weight=F.relu(self.conv(vgg_y_weight))\n",
    "\n",
    "        images_x = self.dec(vgg_x_weight)\n",
    "        images_y = self.dec(vgg_y_weight)\n",
    "        return images_x,images_y\n",
    "\n",
    "    def set_iteration(self,n):\n",
    "        self.iteration=n\n",
    "\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, affine=True):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "\n",
    "        if self.affine:\n",
    "            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n",
    "            self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = [-1] + [1] * (x.dim() - 1)\n",
    "        mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
    "        std = x.view(x.size(0), -1).std(1).view(*shape)\n",
    "        x = (x - mean) / (std + self.eps)\n",
    "\n",
    "        if self.affine:\n",
    "            shape = [1, -1] + [1] * (x.dim() - 2)\n",
    "            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n",
    "        return x\n",
    "\n",
    "class ResBlocks(nn.Module):\n",
    "    def __init__(self, num_blocks, dim, norm='bn', activation='relu', pad_type='zero'):\n",
    "        super(ResBlocks, self).__init__()\n",
    "        self.model = []\n",
    "        for i in range(num_blocks):\n",
    "            self.model += [ResBlock(dim, norm=norm, activation=activation, pad_type=pad_type)]\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# predict masks\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_res, dim, output_dim, activ='relu', pad_type='zero'):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.model = []\n",
    "        # AdaIN residual blocks\n",
    "        self.model += [ResBlocks(n_res, dim, 'bn', activ, pad_type=pad_type)]\n",
    "\n",
    "        self.model += [nn.Upsample(scale_factor=2),\n",
    "                           Conv2dBlock(dim, dim // 2, 5, 1, 2, norm='bn', activation=activ, pad_type='reflect')]\n",
    "        dim //= 2\n",
    "        self.model += [nn.Upsample(scale_factor=2),\n",
    "                           Conv2dBlock(dim, dim // 2, 5, 1, 2, norm='bn', activation=activ, pad_type='reflect')]\n",
    "        dim //= 2\n",
    "        self.model += [nn.Upsample(scale_factor=2),\n",
    "                           Conv2dBlock(dim, dim // 2, 5, 1, 2, norm='bn', activation=activ, pad_type='reflect'),\n",
    "                           ]\n",
    "        dim //= 2\n",
    "        self.model += [nn.Upsample(scale_factor=2),\n",
    "                           Conv2dBlock(dim, dim // 2, 5, 1, 2, norm='bn', activation=activ, pad_type='reflect'),\n",
    "                           ]\n",
    "        dim //= 2\n",
    "        self.model += [nn.Upsample(scale_factor=2),\n",
    "                           Conv2dBlock(dim, dim // 2, 5, 1, 2, norm='bn', activation=activ, pad_type='reflect'),\n",
    "\t\t\t   ]\n",
    "        dim //= 2\n",
    "        # use reflection padding in the last conv layer\n",
    "        self.model += [Conv2dBlock(dim, output_dim, 7, 1, 3, norm='bn', activation='none', pad_type='reflect')]\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "# Basic Blocks\n",
    "##################################################################################\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim, norm='ln', activation='relu', pad_type='zero'):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        model = []\n",
    "        model += [Conv2dBlock(dim ,dim, 3, 1, 1, norm=norm, activation=activation, pad_type=pad_type)]\n",
    "        model += [Conv2dBlock(dim ,dim, 3, 1, 1, norm=norm, activation='none', pad_type=pad_type)]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.model(x)\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "class Conv2dBlock(nn.Module):\n",
    "    def __init__(self, input_dim ,output_dim, kernel_size, stride,\n",
    "                 padding=0, norm='none', activation='relu', pad_type='zero'):\n",
    "        super(Conv2dBlock, self).__init__()\n",
    "        self.use_bias = True\n",
    "        # initialize padding\n",
    "        if pad_type == 'reflect':\n",
    "            self.pad = nn.ReflectionPad2d(padding)\n",
    "        elif pad_type == 'zero':\n",
    "            self.pad = nn.ZeroPad2d(padding)\n",
    "\n",
    "        else:\n",
    "            assert 0, \"Unsupported padding type: {}\".format(pad_type)\n",
    "\n",
    "        # initialize normalization\n",
    "        norm_dim = output_dim\n",
    "        if norm == 'bn':\n",
    "            self.norm = nn.BatchNorm2d(norm_dim)\n",
    "        elif norm == 'in':\n",
    "            self.norm = nn.InstanceNorm2d(norm_dim)\n",
    "        elif norm == 'ln':\n",
    "            self.norm = LayerNorm(norm_dim)\n",
    "        elif norm == 'adain':\n",
    "            self.norm = AdaptiveInstanceNorm2d(norm_dim)\n",
    "        elif norm == 'none':\n",
    "            self.norm = None\n",
    "        else:\n",
    "            assert 0, \"Unsupported normalization: {}\".format(norm)\n",
    "\n",
    "        # initialize activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        elif activation == 'lrelu':\n",
    "            self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
    "        elif activation == 'prelu':\n",
    "            self.activation = nn.PReLU()\n",
    "        elif activation == 'selu':\n",
    "            self.activation = nn.SELU(inplace=True)\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = nn.Softmax(dim=-1)\n",
    "        elif activation == 'none':\n",
    "            self.activation = None\n",
    "        else:\n",
    "            assert 0, \"Unsupported activation: {}\".format(activation)\n",
    "\n",
    "        # initialize convolution\n",
    "        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride, bias=self.use_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(self.pad(x))\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MmpylnUzHSWt"
   },
   "outputs": [],
   "source": [
    "#data load and preprocess\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "# import skimage.io as io\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def get_images(filename):\n",
    "    image_names = np.genfromtxt(filename, dtype=str)\n",
    "    return image_names\n",
    "\n",
    "\n",
    "def load_image(file):\n",
    "    return Image.open(file)\n",
    "\n",
    "\n",
    "class coseg_train_dataset(Dataset):\n",
    "    def __init__(self, data_dir, label_dir, traintxt, input_transform=None, label_transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.input_transform = input_transform\n",
    "        self.label_transform = label_transform\n",
    "        self.traintxt = traintxt\n",
    "        self.train_names = get_images(self.traintxt)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        imagename1 = self.data_dir + self.train_names[index][0] + \".jpg\"\n",
    "        imagename2 = self.data_dir + self.train_names[index][1] + \".jpg\"\n",
    "        labelname1 = self.label_dir + self.train_names[index][2] + \".png\"\n",
    "        labelname2 = self.label_dir + self.train_names[index][3] + \".png\"\n",
    "\n",
    "        with open(imagename1, \"rb\") as f:\n",
    "            image1 = load_image(f).convert('RGB')\n",
    "        with open(imagename2, \"rb\") as f:\n",
    "            image2 = load_image(f).convert('RGB')\n",
    "\n",
    "        with open(labelname1, \"rb\") as f:\n",
    "            label1 = load_image(f).convert('L')\n",
    "        with open(labelname2, \"rb\") as f:\n",
    "            label2 = load_image(f).convert('L')\n",
    "\n",
    "        # random horizontal flip\n",
    "        if random.random() < 0.5:\n",
    "            image1 = image1.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            label1 = label1.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            # image1 = image1.transpose(Image.ROTATE_90)\n",
    "            # label1=label1.transpose(Image.ROTATE_90)\n",
    "\n",
    "        # random horizontal flip\n",
    "        if random.random() < 0.5:\n",
    "            image2 = image2.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            label2 = label2.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            # image2 = image2.transpose(Image.ROTATE_90)\n",
    "            # label2=label2.transpose(Image.ROTATE_90)\n",
    "\n",
    "        if self.input_transform is not None:\n",
    "            image1 = self.input_transform(image1)\n",
    "            image2 = self.input_transform(image2)\n",
    "\n",
    "        if self.label_transform is not None:\n",
    "            label1 = self.label_transform(label1)\n",
    "            label2 = self.label_transform(label2)\n",
    "\n",
    "        return image1, image2, label1, label2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_names)\n",
    "\n",
    "\n",
    "class coseg_val_dataset(Dataset):\n",
    "    def __init__(self, data_dir, label_dir, val_txt, input_transform=None, label_transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.input_transform = input_transform\n",
    "        self.label_transform = label_transform\n",
    "        self.val_txt = val_txt\n",
    "        self.val_names = get_images(self.val_txt)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        imagename1 = self.data_dir + self.val_names[index][0] + \".jpg\"\n",
    "        imagename2 = self.data_dir + self.val_names[index][1] + \".jpg\"\n",
    "        labelname1 = self.label_dir + self.val_names[index][2] + \".png\"\n",
    "        labelname2 = self.label_dir + self.val_names[index][3] + \".png\"\n",
    "\n",
    "        with open(imagename1, \"rb\") as f:\n",
    "            image1 = load_image(f).convert('RGB')\n",
    "        with open(imagename2, \"rb\") as f:\n",
    "            image2 = load_image(f).convert('RGB')\n",
    "\n",
    "        with open(labelname1, \"rb\") as f:\n",
    "            label1 = load_image(f).convert('L')\n",
    "        with open(labelname2, \"rb\") as f:\n",
    "            label2 = load_image(f).convert('L')\n",
    "\n",
    "        if self.input_transform is not None:\n",
    "            image1 = self.input_transform(image1)\n",
    "            image2 = self.input_transform(image2)\n",
    "\n",
    "        if self.label_transform is not None:\n",
    "            label1 = self.label_transform(label1)\n",
    "            label2 = self.label_transform(label2)\n",
    "\n",
    "        return image1, image2, label1, label2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.val_names)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "VFPylndMJQN6",
    "outputId": "44e9d2ae-f2c4-4127-cd34-a5d81e516a24"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `THCTensor_(checkGPU)(state, 4, input, target, output, total_weight)' failed. Some of weight/gradient/input tensors are located on different GPUs. Please move them to a single one. at /pytorch/aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu:65",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b98990e0c36c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-b98990e0c36c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;31m#print('outpu1',output1.shape)#[N, 2, 512, 512]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;31m# calculate loss from output1 and output2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1824\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1826\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1827\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0;31m# dim == 3 or dim > 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `THCTensor_(checkGPU)(state, 4, input, target, output, total_weight)' failed. Some of weight/gradient/input tensors are located on different GPUs. Please move them to a single one. at /pytorch/aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu:65"
     ]
    }
   ],
   "source": [
    "#train setting and process\n",
    "#you may need to change the files' path below\n",
    "train_data_dir=\"/home/guankai/PascalVocCoseg/image/\"\n",
    "train_label_dir=\"/home/guankai/PascalVocCoseg/colabel/train/\"\n",
    "train_txt=\"/home/guankai/PascalVocCoseg/colabel/train.txt\"\n",
    "val_data_dir=\"/home/guankai/PascalVocCoseg/image/\"\n",
    "val_label_dir=\"/home/guankai/PascalVocCoseg/colabel/val/\"\n",
    "val_txt=\"/home/guankai/PascalVocCoseg/colabel/val1600.txt\"\n",
    "\n",
    "# print(os.path.abspath('/home/guankai/PascalVocCoseg/colabel/train.txt'))\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--verbosity\", help=\"increase output verbosity\")\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Attention Based Co-segmentation')\n",
    "parser.add_argument('--lr', default=1e-5, type=float, help='learning rate')\n",
    "parser.add_argument('--weight_decay', default=0.0005,\n",
    "                    help='weight decay value')\n",
    "parser.add_argument('--gpu_ids', default=[0], help='a list of gpus')\n",
    "parser.add_argument('--num_worker', default= 8, help='numbers of worker')\n",
    "parser.add_argument('--batch_size', default=4, help='bacth size')\n",
    "parser.add_argument('--epoches', default=2, help='epoches')\n",
    "\n",
    "parser.add_argument('--train_data', help='training data directory',default=train_data_dir)\n",
    "parser.add_argument('--val_data', help='validation data directory',default=val_data_dir)\n",
    "parser.add_argument('--train_txt', help='training image pair names txt',default=train_txt)\n",
    "parser.add_argument('--val_txt', help='validation image pair names txt',default=val_txt)\n",
    "parser.add_argument('--train_label', help='training label directory',default=train_label_dir)\n",
    "parser.add_argument('--val_label', help='validation label directory',default=val_label_dir)\n",
    "parser.add_argument('--model_path', help='model saving directory',default='model_path/')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# let the label pixels =1 if it >0\n",
    "class Relabel:\n",
    "    def __call__(self, tensor):\n",
    "        assert isinstance(\n",
    "            tensor, torch.LongTensor), 'tensor needs to be LongTensor'\n",
    "        tensor[tensor > 0] = 1\n",
    "        return tensor\n",
    "\n",
    "# numpy -> tensor\n",
    "class ToLabel:\n",
    "    def __call__(self, image):\n",
    "        return torch.from_numpy(np.array(image)).long()\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        self.args = args\n",
    "        self.input_transform = Compose([Resize((512, 512)), ToTensor(), Normalize([.485, .456, .406], [.229, .224, .225])])\n",
    "        self.label_transform = Compose([Resize((512, 512)), CenterCrop(512), ToLabel(), Relabel()])\n",
    "\n",
    "        self.net = model().cuda()\n",
    "        #self.net.turn_off(self.net)\n",
    "        self.net = nn.DataParallel(self.net, device_ids=self.args.gpu_ids)\n",
    "        # self.net.cuda()\n",
    "        self.train_data_loader = DataLoader(coseg_train_dataset(self.args.train_data, self.args.train_label, self.args.train_txt, self.input_transform, self.label_transform),\n",
    "                                            num_workers=self.args.num_worker, batch_size=self.args.batch_size, shuffle=True)\n",
    "        self.val_data_loader = DataLoader(coseg_val_dataset(self.args.val_data, self.args.val_label, self.args.val_txt, self.input_transform, self.label_transform),\n",
    "                                          num_workers=self.args.num_worker, batch_size=self.args.batch_size, shuffle=False)\n",
    "        self.params=filter(lambda p:p.requires_grad,self.net.parameters())\n",
    "        #self.optimizer = optim.Adam(filter(lambda p:p.requires_grad,self.net.parameters()), lr=self.args.lr, weight_decay=self.args.weight_decay)\n",
    "        #weight_decay   权重衰减 --正则化方法之一\n",
    "        self.optimizer=optim.Adam(self.net.parameters(),lr=self.args.lr,weight_decay=self.args.weight_decay)\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    def pixel_accuracy(self, output, label):\n",
    "        correct = len(output[output == label])\n",
    "        wrong = len(output[output != label])\n",
    "        return correct, wrong\n",
    "\n",
    "    def jaccard(self, output, label):\n",
    "        # print('output',output)\n",
    "        # print('label',label)\n",
    "        # ss\n",
    "        temp = output[label == 1]\n",
    "        i = len(temp[temp == 1])\n",
    "        temp = output + label\n",
    "        u = len(temp[temp > 0])\n",
    "        return i, u\n",
    "\n",
    "    def precision(self, output, label):\n",
    "        temp = output[label == 1]\n",
    "        tp = len(temp[temp == 1])\n",
    "        p = len(output[output > 0])\n",
    "        return tp, p\n",
    "\n",
    "    def evaluate(self, net, epoch):\n",
    "        print('--eval')\n",
    "        self.net.eval()\n",
    "        correct = 0\n",
    "        wrong = 0\n",
    "        intersection = 0\n",
    "        union = 0\n",
    "        true_positive = 0\n",
    "        positive = 1\n",
    "        for i, (image1, image2, label1, label2) in enumerate(self.val_data_loader):\n",
    "            image1, image2, label1, label2 = image1.cuda(\n",
    "            ), image2.cuda(), label1.cuda(), label2.cuda()\n",
    "            with torch.no_grad():\n",
    "\n",
    "                output1, output2 = self.net(image1, image2)\n",
    "            output1 = torch.argmax(output1, dim=1)\n",
    "            output2 = torch.argmax(output2, dim=1)\n",
    "\n",
    "            # eval output1\n",
    "            c, w = self.pixel_accuracy(output1, label1)\n",
    "            correct += c\n",
    "            wrong += w\n",
    "\n",
    "            i, u = self.jaccard(output1, label1)\n",
    "            intersection += i\n",
    "            union += u\n",
    "\n",
    "            tp, p = self.precision(output1, label1)\n",
    "            true_positive += tp\n",
    "            positive += p\n",
    "            # eval output2\n",
    "            c, w = self.pixel_accuracy(output2, label2)\n",
    "            correct += c\n",
    "            wrong += w\n",
    "\n",
    "            i, u = self.jaccard(output2, label2)\n",
    "            intersection += i\n",
    "            union += u\n",
    "\n",
    "            tp, p = self.precision(output2, label2)\n",
    "            true_positive += tp\n",
    "            positive += p\n",
    "\n",
    "        print(\"pixel accuracy: {} correct: {}  wrong: {}\".format(\n",
    "            correct / (correct + wrong), correct, wrong))\n",
    "        print(\"precision: {} true_positive: {} positive: {}\".format(\n",
    "            true_positive / positive, true_positive, positive))\n",
    "        print(\"jaccard score: {} intersection: {} union: {}\".format(\n",
    "            intersection / union, intersection, union))\n",
    "        self.net.train()\n",
    "        return correct / (correct + wrong), intersection / union, true_positive / positive\n",
    "\n",
    "    def train(self):\n",
    "        precison_list=[]\n",
    "        jaccard_list=[]\n",
    "        for epoch in range(self.args.epoches):\n",
    "            losses = []\n",
    "            for i, (image1, image2, label1, label2) in enumerate(self.train_data_loader):\n",
    "                image1, image2, label1, label2 = image1.cuda(\n",
    "                ), image2.cuda(), label1.cuda(), label2.cuda()\n",
    "                #print('image1',image1.shape)#[N, 3, 512, 512]\n",
    "\n",
    "\n",
    "                output1, output2 = self.net(image1, image2)\n",
    "\n",
    "\n",
    "                #print('outpu1',output1.shape)#[N, 2, 512, 512]\n",
    "                # calculate loss from output1 and output2\n",
    "                loss = self.loss_func(output1, label1)\n",
    "                loss += self.loss_func(output2, label2)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                losses.append(loss.data.cpu().numpy())\n",
    "\n",
    "                if i % 2000 == 0:\n",
    "                    print(\"---------------------------------------------\")\n",
    "                    print(\"epoch{} iter {}/{} BCE loss:\".format(epoch,\n",
    "                                                                i, len(self.train_data_loader), np.mean(losses)))\n",
    "                    print(\"testing......\")\n",
    "                    acc, jac, pre = self.evaluate(self.net, epoch)\n",
    "                    precison_list.append(pre)\n",
    "                    jaccard_list.append(jac)\n",
    "                    plot_precisonAndjac('', precison_list,jaccard_list)\n",
    "\n",
    "\n",
    "                if i % 2000 == 0 and i != 0:\n",
    "                    torch.save(self.net.state_dict(),\n",
    "                               'epoch{}iter{}.pkl'.format(epoch, i))\n",
    "\n",
    "\n",
    "def plot_precisonAndjac(checkpoint_dir,pre_list,jac_list):\n",
    "    x=range(0,len(pre_list))\n",
    "    y=pre_list\n",
    "    y2=jac_list\n",
    "    plt.switch_backend('agg')\n",
    "    plt.plot(x,y,color='red',marker='o',label='precision')\n",
    "    plt.plot(x,y2,color='blue',marker='o',label='jaccard')\n",
    "    plt.xticks(range(0,len(pre_list)+3,(len(pre_list)+10)//10))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(checkpoint_dir,'precisionAndjac_fig.pdf'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "colab_type": "code",
    "id": "73ZFpV5TDoQe",
    "outputId": "6ec38688-ac10-4f85-8141-871d1e75e3e6"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7b0f1f318942>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mdemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finish!!!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-7b0f1f318942>\u001b[0m in \u001b[0;36msingle_demo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msingle_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mimage1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#Image.fromarray(np.zeros([512,512,3],dtype=np.uint8))#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mimage2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#Image.fromarray(np.zeros([512,512,3],dtype=np.uint8))#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mimage1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "\n",
    "# test phase \n",
    "ckpt = ''\n",
    "test_image_1 = ''\n",
    "test_image_2 = ''\n",
    "\n",
    "class Demo:\n",
    "    def __init__(self):\n",
    "        # self.args = args\n",
    "        self.net = model().cuda()\n",
    "        self.net = nn.DataParallel(self.net, device_ids=[0])\n",
    "        # self.net.load_state_dict(torch.load(ckpt))\n",
    "\n",
    "        self.input_transform = Compose([Resize((512, 512)), ToTensor(\n",
    "        ), Normalize([.485, .456, .406], [.229, .224, .225])])\n",
    "        # self.image1_path = self.args.image1\n",
    "        # self.image2_path = self.args.image2\n",
    "\n",
    "\n",
    "    def pixel_accuracy(self,output,label):\n",
    "        correct=len(output[output==label])\n",
    "        wrong=len(output[output!=label])\n",
    "        return correct,wrong\n",
    "\n",
    "    def jaccard(self,output,label):\n",
    "\n",
    "        temp=output[label==1]\n",
    "        intersection=len(temp[temp==1])\n",
    "        temp=output+label\n",
    "        union=len(temp[temp>0])\n",
    "        return intersection,union\n",
    "\n",
    "    def precision(self,output,label):\n",
    "        temp=output[label==1]\n",
    "        tp=len(temp[temp==1])\n",
    "        p=len(output[output>0])\n",
    "        return tp,p\n",
    "\n",
    "    def single_demo(self):\n",
    "        self.net.eval()\n",
    "        image1 = Image.open(open(test_image_1)).convert('RGB')#Image.fromarray(np.zeros([512,512,3],dtype=np.uint8))#\n",
    "        image2 = Image.open(open(test_image_2)).convert('RGB')#Image.fromarray(np.zeros([512,512,3],dtype=np.uint8))#\n",
    "        image1 = self.input_transform(image1)\n",
    "        image2 = self.input_transform(image2)\n",
    "        image1, image2 = image1.unsqueeze(0).cuda(), image2.unsqueeze(0).cuda()\n",
    "\n",
    "        output1, output2 = self.net(image1, image2)\n",
    "        print('output_origin',output1.shape)\n",
    "\n",
    "        output1=torch.softmax(output1,dim=1,)[:,1,:,:]\n",
    "        output2=torch.softmax(output2,dim=1)[:,1,:,:]\n",
    "\n",
    "        print('output',output1.shape)#[1,512,512]\n",
    "\n",
    "        image1 = (image1 - image1.min()) / image1.max()\n",
    "        image2 = (image2 - image2.min()) / image2.max()\n",
    "        output1=torch.cat([output1,output1,output1],dim=0).unsqueeze(0)\n",
    "        output2=torch.cat([output2,output2,output2],dim=0).unsqueeze(0)\n",
    "        output1 = output1.float().data * 0.8 + image2.data\n",
    "        output2 = output2.float().data * 0.8 + image2.data\n",
    "\n",
    "        print(output1.shape)\n",
    "        print(output2.shape)\n",
    "        npimg = output1[0].cpu().numpy()\n",
    "        # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        npimg = np.hstack([npimg,output2[0].cpu().numpy()])\n",
    "        print(npimg.shape)\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "demo = Demo()\n",
    "demo.single_demo()\n",
    "\n",
    "print(\"Finish!!!\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CoSeg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
